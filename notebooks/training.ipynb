{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b728c6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.9.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: transformers in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.57.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.6.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.2.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.15.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2025.7.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\tsk\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers scikit-learn pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a8ea996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsk\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "RANDOM_SEED = 42\n",
    "MAX_LEN = 128 \n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "MODEL_NAME = \"distilbert-base-uncased\" \n",
    "TARGET_COLUMNS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "NUM_LABELS = len(TARGET_COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3585b5db",
   "metadata": {},
   "source": [
    "Load Preprocessed data and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83d54f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Training comments: 143613, Validation comments: 15958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer saved to ../models\\tokenizer\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "DATA_DIR = '../data'\n",
    "MODELS_DIR = '../models'\n",
    "\n",
    "# Load the data split from the preprocessing notebook\n",
    "X_train = np.load(os.path.join(DATA_DIR, 'X_train.npy'), allow_pickle=True)\n",
    "X_val = np.load(os.path.join(DATA_DIR, 'X_val.npy'), allow_pickle=True)\n",
    "y_train = np.load(os.path.join(DATA_DIR, 'y_train.npy'))\n",
    "y_val = np.load(os.path.join(DATA_DIR, 'y_val.npy'))\n",
    "\n",
    "print(f\"Training comments: {len(X_train)}, Validation comments: {len(X_val)}\")\n",
    "\n",
    "# Load and save the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Save the tokenizer for deployment (Crucial Step!)\n",
    "tokenizer_output_dir = os.path.join(MODELS_DIR, 'tokenizer')\n",
    "if not os.path.exists(tokenizer_output_dir):\n",
    "    os.makedirs(tokenizer_output_dir)\n",
    "tokenizer.save_pretrained(tokenizer_output_dir)\n",
    "\n",
    "print(f\"Tokenizer saved to {tokenizer_output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87906446",
   "metadata": {},
   "source": [
    "Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "200486ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicCommentsDataset(Dataset):\n",
    "    def __init__(self, comments, targets, tokenizer, max_len):\n",
    "        self.comments = comments\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        comment = str(self.comments[item])\n",
    "        target = self.targets[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            comment,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'comment_text': comment,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26e42258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset and DataLoaders created.\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset objects\n",
    "train_dataset = ToxicCommentsDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
    "val_dataset = ToxicCommentsDataset(X_val, y_val, tokenizer, MAX_LEN)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "val_data_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=2\n",
    ")\n",
    "print(\"Dataset and DataLoaders created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb5d2bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ToxicClassifier(nn.Module):\n",
    "    def __init__(self, model_name=\"distilbert-base-uncased\", num_labels=6, dropout_rate=0.1):\n",
    "        super(ToxicClassifier, self).__init__()\n",
    "        # Load the base transformer model\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # Classification head for 6 labels\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use the embedding of the [CLS] token (first token in the sequence)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Initialize the model\n",
    "model = ToxicClassifier(model_name=MODEL_NAME, num_labels=NUM_LABELS)\n",
    "model = model.to(DEVICE)\n",
    "print(\"Model initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aecf0ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hf_xet in c:\\users\\tsk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\tsk\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install hf_xet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fa9e9b",
   "metadata": {},
   "source": [
    "### 5. Training Setup: Optimizer, Scheduler, and LOSS (with Class Weights!) ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11a947df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training components initialized.\n"
     ]
    }
   ],
   "source": [
    "# 5.1. Calculate Class Weights for Imbalanced Data\n",
    "# Calculate positive weight for BCEWithLogitsLoss\n",
    "positive_counts = y_train.sum(axis=0)\n",
    "negative_counts = len(y_train) - positive_counts\n",
    "\n",
    "# Calculate pos_weight for BCEWithLogitsLoss: weight_for_positive = negative_count / positive_count\n",
    "pos_weight = torch.tensor(negative_counts / positive_counts, dtype=torch.float).to(DEVICE)\n",
    "\n",
    "# Loss Function: BCEWithLogitsLoss (ideal for multi-label classification)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Scheduler\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "print(\"Training components initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fe0b1b",
   "metadata": {},
   "source": [
    "Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38401c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, criterion, optimizer, device, scheduler):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    for d in tqdm(data_loader, desc=\"Training\"):\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Clip gradients to prevent exploding gradients\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    \n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for d in tqdm(data_loader, desc=\"Evaluation\"):\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"]\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            loss = criterion(outputs, targets.to(device))\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # Move data back to CPU for metric calculation\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_outputs.extend(torch.sigmoid(outputs).cpu().numpy())\n",
    "            \n",
    "    # Calculate metrics\n",
    "    y_true = np.array(all_targets)\n",
    "    y_pred_proba = np.array(all_outputs)\n",
    "    \n",
    "    # ROC AUC Score (Multi-label)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_proba, average='macro')\n",
    "    \n",
    "    # F1 Score (requires converting probabilities to binary predictions)\n",
    "    # Using 0.5 as threshold for F1\n",
    "    y_pred_binary = (y_pred_proba >= 0.5).astype(int)\n",
    "    f1_macro = f1_score(y_true, y_pred_binary, average='macro')\n",
    "\n",
    "    return np.mean(losses), roc_auc, f1_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e8c2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1/3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/4488 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "history = pd.DataFrame(columns=['epoch', 'train_loss', 'val_loss', 'val_roc_auc', 'val_f1_macro'])\n",
    "best_roc_auc = 0\n",
    "MODEL_PATH = os.path.join(MODELS_DIR, 'toxic_classifier.pth')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n--- Epoch {epoch + 1}/{EPOCHS} ---\")\n",
    "    \n",
    "    # Training\n",
    "    train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        DEVICE,\n",
    "        scheduler\n",
    "    )\n",
    "    \n",
    "    # Evaluation\n",
    "    val_loss, val_roc_auc, val_f1_macro = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        criterion,\n",
    "        DEVICE\n",
    "    )\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Val ROC AUC (Macro): {val_roc_auc:.4f}\")\n",
    "    print(f\"Val F1 Score (Macro): {val_f1_macro:.4f}\")\n",
    "    \n",
    "    # Save the best model based on validation ROC AUC\n",
    "    if val_roc_auc > best_roc_auc:\n",
    "        torch.save(model.state_dict(), MODEL_PATH)\n",
    "        best_roc_auc = val_roc_auc\n",
    "        print(f\"*** Model saved! New best ROC AUC: {best_roc_auc:.4f} ***\")\n",
    "\n",
    "    # Record history\n",
    "    history.loc[epoch] = [epoch + 1, train_loss, val_loss, val_roc_auc, val_f1_macro]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc3c96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Training Complete ---\")\n",
    "print(f\"Best Validation ROC AUC achieved: {best_roc_auc:.4f}\")\n",
    "print(f\"Trained model state_dict saved to: {MODEL_PATH}\")\n",
    "print(f\"Tokenizer saved to: {tokenizer_output_dir}\")\n",
    "\n",
    "# Display training history\n",
    "print(\"\\nTraining History:\")\n",
    "display(history)\n",
    "\n",
    "# Final check: Load the best model and evaluate\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "final_loss, final_roc_auc, final_f1_macro = eval_model(model, val_data_loader, criterion, DEVICE)\n",
    "\n",
    "print(\"\\n--- Final Evaluation with Best Model ---\")\n",
    "print(f\"Final Val ROC AUC: {final_roc_auc:.4f}\")\n",
    "print(f\"Final Val F1 Macro: {final_f1_macro:.4f}\")\n",
    "\n",
    "# Push artifacts to GitHub if repository is cloned (Manual step in Colab)\n",
    "# print(\"\\nRun the following commands in Colab to commit artifacts:\")\n",
    "# print(f\"!git add {MODEL_PATH}\")\n",
    "# print(f\"!git add {tokenizer_output_dir}/*\")\n",
    "# print(f\"!git add notebooks/02_PyTorch_Training.ipynb\")\n",
    "# print('!git commit -m \"FEAT: Trained new PyTorch toxic classifier\"')\n",
    "# print('!git push origin main')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
